matrix <- (c(784,1793,1147,1229,902,1747,1171,1261),nrow=2,byrow=TRUE)
library(rattle)
rattle()
install(Java)
install.packages("rJava")
install.packages('mice')
# Or non-adjacent elements
matrix5[c("Zack","Kelly"),"Exam2"]
library(rattle)
rattle()
library(swirl)
swirl()
1
15:1
library(swirl)
swirl()
Is()
ls()
class(plants)
dim(plants)
nrow(plants)
ncol(plants)
object.size(plants)
names(plants)
head()
head(plants)
head(10)
head(plants,10)
tail(plants,15)
summary(plants)
table(plants$Active_Growth_Period)
str(plants)
refelction<-"Looking at Data helped me reviewed functions to exam the data, such as dim, and taught me how to extract the rows that i want from a data like head(plants,10)."
refelction<-"Looking at Data helped me to review functions to exam the data, such as dim, and taught me how to extract the rows that i want from a data like head(plants,10)."
save.image("~/Desktop/Li_tutorial12.rda.RData")
rm(list=ls())
library(swirl)
swirl()
getwd()
ls()
x <- 9
ls()
list.files()
help()
?list.files
args()
args(list.files())
args(list.files)
setwd("old.dir")
old.dir <- getwd()
dir.create("testdir")
setwd("testdir")
file.create("mytest.R")
list.files
list.files()
file.exists("mytest.R")
file.info("mytest.R")
file.rename("mytest2.R")
file.rename("mytest.R","mytest2.R")
file.copy("mytest2.R","mytest3.R")
file.path("mytest3.R")
file.path("folder1","folder2")
?dir.create
dir.create(file.path("testdir2","testdir3"), recursive=TRUE)
setwd(old.dir)
save.image("~/Desktop/Li_tutorial2.rda.RData")
reflection<-"Workspace and Files taught me how to rename a file, make a copy for a file, and create a new working directory."
save.image("~/Desktop/Li_tutorial2.rda.RData")
library(rattle)
rattle()
library(rattle)
rattle()
library(rattle)
library(rattle)
rattle()
library(rattle)
rattle()
library(swirl)
swirl()
X <- c(44,NA,5,NA)
x <- c(44,NA,5,NA)
x*3
y <- rnorm(1000)
z <- rep(NA,1000)
my_data <- sample(c(y,z),100)
is.na()
is.na(my_data)
my_na <- is.na(my_data)
my_na
my_data == NA
sum(my_na)
my_data
0/0
Inf-Inf
reflection <- "Missing Values taught me to be careful when using the "==" operator to find NAs and we can use sum() to find the NAs in our data."
reflection<-"Missing Values taught me to be careful when using the "==" operator to find NAs and we can use sum() to find the NAs in our data."
reflection
reflection<-"Missing Values taught me to be careful when using the logical expression to find NAs and we can use sum() to find the NAs in our data."
save.image("~/Desktop/R/Li_tutorial5.rda.RData")
rm(list=ls())
library(swirl())
library(swir)
library(swirl)
library(swirl)
swirl()
d1 <- Sys.date()
d1 <- Sys.Date()
class(d1)
unclass(d1)
d1
d2 <- as.Date("1969-01-01")
unclass(d2)
t1 <- Sys.time(2022-02-22)
t1 <- Sys.time()
t1
class(t1)
unclass(t1)
t2 <- as.POSIXct(Sys.time())
t2 <- as.POSIXlt(Sys.time())
class(t2)
t2
unclass(te)
unclass(t2)
str(unclass(t2))
t2$min
weekdays(d1)
months(t1)
quarters(t2)
t3 <- "October 17,1986 08:24"
t3 <- "October 17, 1986 08:24"
strptime(t3, "%B %d, %Y %H:%M")
t4 <- strptime(t3, "%B %d, %Y %H:%M")
t4
class(t4)
Sys.time() > t1
Sys.time() - t1
difftime(Sys.time(), t1, units = 'days')
reflection <- "Dates and Times taught me how to use unclass to get the exact number of days from a certain day in the past to today and functions to return weekdays, months, and quarters."
save.image("~/Desktop/R/Li_tutorial14.rda.RData")
load("~/Desktop/R/Li_tutorial14.rda.RData")
getwd()
library(rattle)
rattle()
rattle()
library(rattle)
library(rattle)
rattle()
library(rattle)
rattle()
library(rattle)
rattle()
quotes<-character(0)
authors<-character(0)
author_urls<-character(0)
tags<-character(0)
for (i in 1:10){
print(i)
Sys.sleep(5) #
library(xml2)
quotes<-character(0)
authors<-character(0)
author_urls<-character(0)
tags<-character(0)
for (i in 1:10){
print(i)
Sys.sleep(5)
url<-paste("http://quotes.toscrape.com/page/", i, sep="")
page<-read_html(url)
quote<-xml_text(xml_find_all(page,
"//div[@class='quote']/span[@class='text']"))
quotes<-c(quotes, quote) # Append to quotes vector
author<-xml_text(xml_find_all(page,
"//div[@class='quote']//small[@class='author']"))
authors<-c(authors, author)
author_url<-xml_attr(xml_find_all(page,
"//div[@class='quote']/span/a"),
"href")
rm(list=ls())
qqq
rm(list=ls())
library(xml2)
###exercise 1
head(books)
books$url2 <- paste("http://books.toscrape.com/catalogue/",books$url,sep='')
##books$star2 <- ifelse(books$star=='star-rating One',1
#ifelse(books$star=='star-rating Two'),2
#ifelse(books$star=='star-rating Three'),3
books$star[books$star=='star-rating One'] <- 1
books$star[books$star=='star-rating Two'] <- 2
books$star[books$star=='star-rating Three'] <- 3
books$star[books$star=='star-rating Four'] <- 4
books$star[books$star=='star-rating Five'] <- 5
head(books)
books$price<-as.numeric(gsub("[£]","",books$price))
books$availability<-trimws(books$availability)
books$availability
head(books)
###exercise 2
#create a empty vectors to store all urls, stars, etc
url2<-character(0)
star2<-character(0)
price2<-character(0)
title2<-character(0)
availability2<-character(0)
for(i in 1:50){
link <- paste("http://books.toscrape.com/catalogue/page-",i,".html",sep="")
print(link)
page<-read_html(link)
url <- xml_attr(xml_find_all(page,"//article[@class='product_pod']/div[@class='image_container']/a"),'href')
url2<-c(url2,url)
title<- xml_attr(xml_find_all(page,"//article[@class='product_pod']/div[@class='image_container']/a/img"),'alt')
title2<-c(title2,title)
star <- xml_attr(xml_find_all(page,"//article[@class='product_pod']/p"),"class")
star2<-c(star2,star)
price <- xml_text(xml_find_all(page,"//article[@class='product_pod']/div[@class='product_price']/p[@class='price_color']"))
price2<-c(price2,price)
availability <- xml_text(xml_find_all(page,"//article[@class='product_pod']/div[@class='product_price']/p[@class='instock availability']"))
availability2<-c(availability2,availability)
}
allBooks <-data.frame(url2,title2,star2,price2,availability2)
###exercise 3
page <- read_html("http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html")
xml_find_all(page,"//table/tr/td")
UPC<-xml_text(xml_find_all(page,"//table/tr[1]/td"))
UPC
product_type<-xml_text(xml_find_all(page,"//table/tr[2]/td"))
product_type
reviews<-xml_text(xml_find_all(page,"//table/tr[7]/td"))
reviews
###exercise 4
xml_text(xml_find_all(page,"//table/tr[6]/td"))
##remove anything but not numbers
copies<-gsub("[^0-9]","",xml_text(xml_find_all(page,"//table/tr[6]/td")))
copies
###exercise 5
head(books)
descriptions<-character(0)
for(i in books$url){
link<-paste("http://books.toscrape.com/",i,sep="")
print(link)
page<-read_html(link)
description<-xml_text(xml_find_all(page,"//div[@id='product_description']/following-sibling::p"))
descriptions<-c(descriptions,description)
}
rm(list=ls())
### Lab 8 ###
## Template ##
library(xml2)
page<-read_html("http://books.toscrape.com/")
class(page) # Result is class xml_document
page # Print HTML to console
# Data is arranged in <div> tags with class "quote"
product<-xml_find_all(page, "//article[@class='product_pod']")
class(product)
length(product) # 20 products on this page
# Scrape URLs
url<-xml_attr(xml_find_all(page,
"//article[@class='product_pod']/div[@class='image_container']/a"),
"href")
length(url)
url # Partial URLs. Extend later
# Full titles in title attribute of <a>
title<-xml_attr(xml_find_all(page,
"//article[@class='product_pod']/h3/a"),
"title")
length(title)
title
# Stars stored as text in class label of <p>
star<-xml_attr(xml_find_all(page,
"//article[@class='product_pod']/p"),
"class")
length(star)
star # Clean data later and transform to integer
# Prices are in first <p> tag within <div>
price<-xml_text(xml_find_all(page,
"//article[@class='product_pod']/div[@class='product_price']/p[1]"))
length(price)
price # Clean data later and transform to integer
# Availability in <p> with class label
availability<-xml_text(xml_find_all(page,
"//article[@class='product_pod']//p[@class='instock availability']"))
length(availability)
availability # Clean data later
# Save as dataframe:
books<-data.frame(url, title, star, price, availability)
rm(list=ls())
library(xml2)
###exercise 1
head(books)
books$url2 <- paste("http://books.toscrape.com/catalogue/",books$url,sep='')
##books$star2 <- ifelse(books$star=='star-rating One',1
#ifelse(books$star=='star-rating Two'),2
#ifelse(books$star=='star-rating Three'),3
books$star[books$star=='star-rating One'] <- 1
books$star[books$star=='star-rating Two'] <- 2
books$star[books$star=='star-rating Three'] <- 3
books$star[books$star=='star-rating Four'] <- 4
books$star[books$star=='star-rating Five'] <- 5
head(books)
books$price<-as.numeric(gsub("[£]","",books$price))
books$availability<-trimws(books$availability)
books$availability
head(books)
###exercise 2
#create a empty vectors to store all urls, stars, etc
url2<-character(0)
star2<-character(0)
price2<-character(0)
title2<-character(0)
availability2<-character(0)
for(i in 1:50){
link <- paste("http://books.toscrape.com/catalogue/page-",i,".html",sep="")
print(link)
page<-read_html(link)
url <- xml_attr(xml_find_all(page,"//article[@class='product_pod']/div[@class='image_container']/a"),'href')
url2<-c(url2,url)
title<- xml_attr(xml_find_all(page,"//article[@class='product_pod']/div[@class='image_container']/a/img"),'alt')
title2<-c(title2,title)
star <- xml_attr(xml_find_all(page,"//article[@class='product_pod']/p"),"class")
star2<-c(star2,star)
price <- xml_text(xml_find_all(page,"//article[@class='product_pod']/div[@class='product_price']/p[@class='price_color']"))
price2<-c(price2,price)
availability <- xml_text(xml_find_all(page,"//article[@class='product_pod']/div[@class='product_price']/p[@class='instock availability']"))
availability2<-c(availability2,availability)
}
allBooks <-data.frame(url2,title2,star2,price2,availability2)
###exercise 3
page <- read_html("http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html")
xml_find_all(page,"//table/tr/td")
UPC<-xml_text(xml_find_all(page,"//table/tr[1]/td"))
UPC
product_type<-xml_text(xml_find_all(page,"//table/tr[2]/td"))
product_type
reviews<-xml_text(xml_find_all(page,"//table/tr[7]/td"))
reviews
###exercise 4
xml_text(xml_find_all(page,"//table/tr[6]/td"))
##remove anything but not numbers
copies<-gsub("[^0-9]","",xml_text(xml_find_all(page,"//table/tr[6]/td")))
copies
###exercise 5
head(books)
descriptions<-character(0)
for(i in books$url){
link<-paste("http://books.toscrape.com/",i,sep="")
print(link)
page<-read_html(link)
description<-xml_text(xml_find_all(page,"//div[@id='product_description']/following-sibling::p"))
descriptions<-c(descriptions,description)
}
rm(list=ls())
library(swirl)
swirl()
head(flags)
dim(flags)
class(flags)
cls_list <- lapply(flags,class)
cls_list
class(cls_list)
as.character(cls_list)
sapply(lapply())
play()
nxt()
?sapply(list, function)
?sapply()
cls_vect<-sapply(flags,class)
class(cls_vect)
sum(flags$orange)
flag_colors<-flags[,11:17]
head(flag_colors)
lapply(flag_colors,sum)
sapply(flag_colors,sum)
sapply(flag_colors,mean)
flag_shapes<-flags[,19:23]
lapply(flag_shapes,range)
sapply(flag_shapes,range)
shape_mat <- sapply(flag_shapes,range)
View(shape_mat)
shape_mat
class(shape_mat)
unique(c(3,4,5,5,5,6,6,))
unique(c(3,4,5,5,5,6,6))
unique_vals<-lapply(flags,unique)
unique_vals
sapply(unique_vals,length)
sapply(flags,unique)
lapply(unique_vals, function(elem) elem[2])
reflection<-"Lapply and Sapply taught me how to return a list object of same length."
save.image("~/Desktop/Li_tutorial10.RData")
library(swirl)
swirl()
sapply(flags,unique)
vapply(flags,unique,numeric(1))
ok()
sapply(flags,class)
vapply(flags,class,character(1))
?tapply
table(flags$landmass)
table(flags$animate)
tapply(flags$animate,flags$landmass,mean)
tapply(flags$population,flags$red,summary)
tapply(flags$population,flags$landmass,summary)
reflection<-"Vapply and Tapply taught me that vapply can show the error that sapply cannot and how to split data into groups by using tapply. "
save.image("~/Desktop/R/Li_tutorial11.RData")
library(rattle)
rattle()
library(rattle)
rattle()
library(Quandl)
platinum<-Quandl("LPPM/PLAT")
all_temperatures<-Quandl("http://iot-bais.azurewebsites.net/temperatures")
library(RSocrata)
library(RSocrata)
url<-"http://iot-bais.azurewebsites.net/temperatures"
expenditures<-read.socrata(url)
install.packages("RSocrata")
install.packages("RSocrata")
library(RSocrata)
url<-"http://iot-bais.azurewebsites.net/temperatures"
expenditures<-read.socrata(url)
library(httr)
url<-"http://iot-bais.azurewebsites.net/temperatures"
rm(list=ls())
raw<-GET(url)
GET(url)
rm(list=ls())
install.packages('httr')
install.packages("httr")
library(httr)
url<-"http://iot-bais.azurewebsites.net/temperatures"
raw<-GET(url)
View(raw)
library(jsonlite)
content(raw,"text")
all_temperature<-fromJSON(content(raw,"text"),flatten=TRUE)
head(all_temperature)
all_temperature$readTime<-as.POSIXlt(all_temperature$readTime,
format="%Y-%m-%dT%H:%M",
tz = "America/Chicago")
str(all_temperature)
View(all_temperature)
min_temp<-min(all_temperature$temperature)
min_temp
min_temp<-min(all_temperature$temperature)
min_temp
max_temp<-max(all_temperature$temperature)
max_temp
mean_temp<-mean(all_temperature$temperature)
mean_temp
library(rattle)
rattle()
getwd()
library(rattle)
rattle()
library(swirl)
swirl()
swirl()
data(cars)
?cars
head(cars)
plot(cars)
?plot()
?plot
plot(x=cars$speed,cars$dist)
plot(cars$dist,cars$speed)
plot(x=cars$speed,cars$dist,xlab="Speed")
plot(x=cars$speed,cars$dist,xlab="Speed",ylab = "Stopping Distance")
plot(x=cars$speed,cars$dist,ylab = "Stopping Distance")
plot(x=cars$speed,cars$dist,xlab="Speed",ylab = "Stopping Distance")
plot(cars,main="My Plot")
plot(cars, sub="My Plot Subtitle")
plot(cars, col=2)
plot(cars, xlim = c(10, 15))
plot(cars, pch = 2)
data(mtcars)
?boxplot
boxplot(formula = mpg ~ cyl, data = mtcars)
hist(mtcars$mpg)
reflection<-"Base Graphics taguht me how to plot graphs and add labels by applying different functions."
save.image("~/Desktop/Li_tutorial15.RData")
getwd()
library(rattle)
rattle()
library(rattle)
rattle()
getwd()
library(rattle)
rattle
rattle()
rattle()
library(rattle)
rattle()
rattle()
rm(list=ls())
library(httr)
library(jsonlite)
####################################Read Data##################################
url<-"http://iot-bais.azurewebsites.net/temperatures"
temperature<-GET(url)
class(temperature) # Object of class "response"
temperature$status_code
content(temperature, "text") # ?content Extract content from request
# Parse JSON from API response
all_temperatures<-fromJSON(content(temperature, "text"), flatten=TRUE)
head(all_temperatures)
rm(list=ls())
data<- read.csv("Spotify_Final_Data.csv")
getwd()
setwd("/Users/zilingli/Documents/GITHUB/R Project")
setwd("/Users/zilingli/Documents/GitHub/Data-Wrangling-Project-")
data<- read.csv("Spotify_Final_Data.csv")
View(data)
